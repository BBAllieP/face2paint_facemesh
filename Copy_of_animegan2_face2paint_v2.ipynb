{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of animegan2_face2paint_v2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BBAllieP/face2paint_facemesh/blob/main/Copy_of_animegan2_face2paint_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYheet4TKrL_",
        "outputId": "6f381d7a-ae6b-41cf-c97c-d6140fccd750"
      },
      "source": [
        "!pip install mediapipe"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 52 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.37.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.15.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.4.7)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3iD1EyICS48",
        "outputId": "898a6315-dec9-4329-da39-be4609137e62"
      },
      "source": [
        "#@title AnimeGAN model from https://github.com/bryandlee/animegan2-pytorch\n",
        "! git clone https://github.com/bryandlee/animegan2-pytorch\n",
        "\n",
        "model_fname = \"face_paint_512_v2_0.pt\"\n",
        "\n",
        "model_urls = {\n",
        "    \"face_paint_512_v0.pt\": \"https://drive.google.com/uc?id=1WK5Mdt6mwlcsqCZMHkCUSDJxN1UyFi0-\",\n",
        "    \"face_paint_512_v2_0.pt\": \"https://drive.google.com/uc?id=18H3iK09_d54qEDoWIc82SyWB2xun4gjU\",\n",
        "}\n",
        "\n",
        "! gdown {model_urls[model_fname]}\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"animegan2-pytorch\")\n",
        "\n",
        "import torch\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "from model import Generator\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "model = Generator().eval().to(device)\n",
        "model.load_state_dict(torch.load(model_fname))\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
        "\n",
        "def face2paint(\n",
        "    img: Image.Image,\n",
        "    size: int,\n",
        "    side_by_side: bool = True,\n",
        ") -> Image.Image:\n",
        "\n",
        "    w, h = img.size\n",
        "    s = min(w, h)\n",
        "    img = img.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
        "    img = img.resize((size, size), Image.LANCZOS)\n",
        "\n",
        "    input = to_tensor(img).unsqueeze(0) * 2 - 1\n",
        "    output = model(input.to(device)).cpu()[0]\n",
        "\n",
        "    if side_by_side:\n",
        "        output = torch.cat([input[0], output], dim=2)\n",
        "\n",
        "    output = (output * 0.5 + 0.5).clip(0, 1)\n",
        "\n",
        "    return to_pil_image(output)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'animegan2-pytorch'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (184/184), done.\u001b[K\n",
            "remote: Compressing objects: 100% (164/164), done.\u001b[K\n",
            "remote: Total 184 (delta 89), reused 74 (delta 19), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (184/184), 37.77 MiB | 28.69 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18H3iK09_d54qEDoWIc82SyWB2xun4gjU\n",
            "To: /content/face_paint_512_v2_0.pt\n",
            "100% 8.60M/8.60M [00:00<00:00, 136MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApR-nTcGUXHN"
      },
      "source": [
        "#@title Face Detector & FFHQ-style Alignment\n",
        "\n",
        "# https://github.com/woctezuma/stylegan2-projecting-images\n",
        "\n",
        "import os\n",
        "import dlib\n",
        "import collections\n",
        "from typing import Union, List\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_dlib_face_detector(predictor_path: str = \"shape_predictor_68_face_landmarks.dat\"):\n",
        "\n",
        "    if not os.path.isfile(predictor_path):\n",
        "        model_file = \"shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "        os.system(f\"wget http://dlib.net/files/{model_file}\")\n",
        "        os.system(f\"bzip2 -dk {model_file}\")\n",
        "\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    shape_predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "    def detect_face_landmarks(img: Union[Image.Image, np.ndarray]):\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "        faces = []\n",
        "        dets = detector(img)\n",
        "        for d in dets:\n",
        "            shape = shape_predictor(img, d)\n",
        "            faces.append(np.array([[v.x, v.y] for v in shape.parts()]))\n",
        "        return faces\n",
        "    \n",
        "    return detect_face_landmarks\n",
        "\n",
        "\n",
        "def display_facial_landmarks(\n",
        "    img: Image, \n",
        "    landmarks: List[np.ndarray],\n",
        "    fig_size=[15, 15]\n",
        "):\n",
        "    plot_style = dict(\n",
        "        marker='o',\n",
        "        markersize=4,\n",
        "        linestyle='-',\n",
        "        lw=2\n",
        "    )\n",
        "    pred_type = collections.namedtuple('prediction_type', ['slice', 'color'])\n",
        "    pred_types = {\n",
        "        'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\n",
        "        'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\n",
        "        'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\n",
        "        'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\n",
        "        'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\n",
        "        'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\n",
        "        'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\n",
        "        'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\n",
        "        'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\n",
        "    }\n",
        "\n",
        "    fig = plt.figure(figsize=fig_size)\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "\n",
        "    for face in landmarks:\n",
        "        for pred_type in pred_types.values():\n",
        "            ax.plot(\n",
        "                face[pred_type.slice, 0],\n",
        "                face[pred_type.slice, 1],\n",
        "                color=pred_type.color, **plot_style\n",
        "            )\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n",
        "\n",
        "import PIL.Image\n",
        "import PIL.ImageFile\n",
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "\n",
        "\n",
        "def align_and_crop_face(\n",
        "    img: Image.Image,\n",
        "    landmarks: np.ndarray,\n",
        "    expand: float = 1.0,\n",
        "    output_size: int = 1024, \n",
        "    transform_size: int = 4096,\n",
        "    enable_padding: bool = True,\n",
        "):\n",
        "    # Parse landmarks.\n",
        "    # pylint: disable=unused-variable\n",
        "    lm = landmarks\n",
        "    lm_chin          = lm[0  : 17]  # left-right\n",
        "    lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "    lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "    lm_nose          = lm[27 : 31]  # top-down\n",
        "    lm_nostrils      = lm[31 : 36]  # top-down\n",
        "    lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "    lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "    lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "    lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "    # Calculate auxiliary vectors.\n",
        "    eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "    print(eye_left)\n",
        "    eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "    print(eye_right)\n",
        "    eye_avg      = (eye_left + eye_right) * 0.5\n",
        "    print(eye_avg)\n",
        "    eye_to_eye   = eye_right - eye_left\n",
        "    mouth_left   = lm_mouth_outer[0]\n",
        "    mouth_right  = lm_mouth_outer[6]\n",
        "    mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "    eye_to_mouth = mouth_avg - eye_avg\n",
        "    print(eye_to_mouth)\n",
        "\n",
        "    # Choose oriented crop rectangle.\n",
        "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "    x /= np.hypot(*x)\n",
        "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "    x *= expand\n",
        "    y = np.flipud(x) * [-1, 1]\n",
        "    c = eye_avg + eye_to_mouth * 0.1\n",
        "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "    qsize = np.hypot(*x) * 2\n",
        "\n",
        "    # Shrink.\n",
        "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "    if shrink > 1:\n",
        "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "        quad /= shrink\n",
        "        qsize /= shrink\n",
        "\n",
        "    # Crop.\n",
        "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "    crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "        img = img.crop(crop)\n",
        "        quad -= crop[0:2]\n",
        "\n",
        "    # Pad.\n",
        "    pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "    if enable_padding and max(pad) > border - 4:\n",
        "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "        h, w, _ = img.shape\n",
        "        y, x, _ = np.ogrid[:h, :w, :1]\n",
        "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "        blur = qsize * 0.02\n",
        "        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "        img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "        quad += pad[:2]\n",
        "\n",
        "    # Transform.\n",
        "    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "    if output_size < transform_size:\n",
        "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "    return img\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6FSeCexLIjk"
      },
      "source": [
        "import mediapipe as mp\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "# Load drawing_utils and drawing_styles\n",
        "mp_drawing = mp.solutions.drawing_utils \n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5kHl8tJnwfX"
      },
      "source": [
        "import mediapipe as mp\n",
        "def align_and_crop_face_mp(\n",
        "    img: Image.Image,\n",
        "    det: mp.framework.formats.detection_pb2.Detection,\n",
        "    expand: float = 1.0,\n",
        "    output_size: int = 1024, \n",
        "    transform_size: int = 4096,\n",
        "    enable_padding: bool = True,\n",
        "):\n",
        "\n",
        "    # Calculate auxiliary vectors.\n",
        "    eye_left_s     = mp_face_detection.get_key_point(det, mp_face_detection.FaceKeyPoint.LEFT_EYE)\n",
        "    eye_left = np.array([eye_left_s.x*img.width,eye_left_s.y*img.height])\n",
        "    eye_right_s    = mp_face_detection.get_key_point(det, mp_face_detection.FaceKeyPoint.RIGHT_EYE)\n",
        "    eye_right = np.array([eye_right_s.x*img.width,eye_right_s.y*img.height])\n",
        "    eye_avg      = (eye_left + eye_right) * 0.5\n",
        "    eye_to_eye   = eye_right - eye_left\n",
        "    mouth_avg_s    = mp_face_detection.get_key_point(det, mp_face_detection.FaceKeyPoint.MOUTH_CENTER)\n",
        "    mouth_avg = np.array([mouth_avg_s.x*img.width,mouth_avg_s.y*img.height])\n",
        "    eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "    # Choose oriented crop rectangle.\n",
        "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "    x /= np.hypot(*x)\n",
        "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "    x *= expand\n",
        "    y = np.flipud(x) * [-1, 1]\n",
        "    c = eye_avg + eye_to_mouth * 0.1\n",
        "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "    qsize = np.hypot(*x) * 2\n",
        "\n",
        "    # Shrink.\n",
        "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "    if shrink > 1:\n",
        "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "        quad /= shrink\n",
        "        qsize /= shrink\n",
        "\n",
        "    # Crop.\n",
        "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "    crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "        img = img.crop(crop)\n",
        "        quad -= crop[0:2]\n",
        "\n",
        "    # Pad.\n",
        "    pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "    if enable_padding and max(pad) > border - 4:\n",
        "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "        h, w, _ = img.shape\n",
        "        y, x, _ = np.ogrid[:h, :w, :1]\n",
        "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "        blur = qsize * 0.02\n",
        "        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "        img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "        quad += pad[:2]\n",
        "\n",
        "    # Transform.\n",
        "    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "    if output_size < transform_size:\n",
        "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "    img = img.transpose(PIL.Image.FLIP_TOP_BOTTOM).transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "    return img"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gB4V0lYuGtM"
      },
      "source": [
        "import mediapipe as mp\n",
        "def align_and_crop_face_fm(\n",
        "    img: Image.Image,\n",
        "    lm: mp.framework.formats.landmark_pb2,\n",
        "    expand: float = 1.0,\n",
        "    output_size: int = 1024, \n",
        "    transform_size: int = 4096,\n",
        "    enable_padding: bool = True,\n",
        "):\n",
        "    ## between eyes: point 6\n",
        "    ## Left eye: 163-362\n",
        "    ## right eye: 133-35\n",
        "    ## mouth: point 13\n",
        "    # Calculate auxiliary vectors.\n",
        "    eye_left_r     = np.array([lm.landmark[362].x*img.width, lm.landmark[362].y*img.height])\n",
        "    eye_left_l     = np.array([lm.landmark[163].x*img.width, lm.landmark[163].y*img.height])\n",
        "    eye_left = (eye_left_r+eye_left_l) * 0.5\n",
        "    eye_right_r     = np.array([lm.landmark[133].x*img.width, lm.landmark[133].y*img.height])\n",
        "    eye_right_l     = np.array([lm.landmark[35].x*img.width, lm.landmark[35].y*img.height])\n",
        "    eye_right = (eye_right_r+eye_right_l) * 0.5\n",
        "    eye_avg      = (eye_left + eye_right) * 0.5\n",
        "    eye_to_eye   = eye_right - eye_left\n",
        "    mouth_avg    = np.array([lm.landmark[13].x*img.width, lm.landmark[13].y*img.height])\n",
        "    eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "    # Choose oriented crop rectangle.\n",
        "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "    x /= np.hypot(*x)\n",
        "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "    x *= expand\n",
        "    y = np.flipud(x) * [-1, 1]\n",
        "    c = eye_avg + eye_to_mouth * 0.1\n",
        "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "    qsize = np.hypot(*x) * 2\n",
        "\n",
        "    # Shrink.\n",
        "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "    if shrink > 1:\n",
        "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "        quad /= shrink\n",
        "        qsize /= shrink\n",
        "\n",
        "    # Crop.\n",
        "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "    crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "        img = img.crop(crop)\n",
        "        quad -= crop[0:2]\n",
        "\n",
        "    # Pad.\n",
        "    pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "    if enable_padding and max(pad) > border - 4:\n",
        "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "        h, w, _ = img.shape\n",
        "        y, x, _ = np.ogrid[:h, :w, :1]\n",
        "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "        blur = qsize * 0.02\n",
        "        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "        img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "        quad += pad[:2]\n",
        "\n",
        "    # Transform.\n",
        "    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "    if output_size < transform_size:\n",
        "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "    #img = img.transpose(PIL.Image.FLIP_TOP_BOTTOM).transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "    return img"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFZQ0l1zHXYt",
        "outputId": "56282063-73bf-4c1a-a4fb-8c24f7fcb19d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUoLBARyHTi1"
      },
      "source": [
        "#@markdown Upload an image to the encoder4editing folder and set the image_name into the image name\n",
        "image_name = '' #@param {type:\"string\"}"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Ev3Qn1y36x"
      },
      "source": [
        "image = cv2.imread(image_name)\n",
        "resize_and_show(image)\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "mp_face_detection = mp.solutions.face_detection\n",
        "# Load drawing_utils and drawing_styles\n",
        "mp_drawing = mp.solutions.drawing_utils \n",
        "mp_drawing_styles = mp.solutions.drawing_styles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQKsablYy024"
      },
      "source": [
        "# Run MediaPipe Face Mesh.\n",
        "with mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=True,\n",
        "    refine_landmarks=True,\n",
        "    max_num_faces=1,\n",
        "    min_detection_confidence=0.5) as face_mesh:\n",
        "    annotated_image = image.copy()\n",
        "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    if results.multi_face_landmarks:\n",
        "      for face_landmarks in results.multi_face_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "          image=annotated_image,\n",
        "          landmark_list=face_landmarks,\n",
        "          connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
        "          landmark_drawing_spec=None,\n",
        "          connection_drawing_spec=mp_drawing_styles\n",
        "          .get_default_face_mesh_tesselation_style())\n",
        "        mp_drawing.draw_landmarks(\n",
        "            image=annotated_image,\n",
        "            landmark_list=face_landmarks,\n",
        "            connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
        "            landmark_drawing_spec=None,\n",
        "            connection_drawing_spec=mp_drawing_styles\n",
        "            .get_default_face_mesh_contours_style())\n",
        "        mp_drawing.draw_landmarks(\n",
        "            image=annotated_image,\n",
        "            landmark_list=face_landmarks,\n",
        "            connections=mp_face_mesh.FACEMESH_IRISES,\n",
        "            landmark_drawing_spec=None,\n",
        "            connection_drawing_spec=mp_drawing_styles\n",
        "            .get_default_face_mesh_iris_connections_style())\n",
        "        resize_and_show(annotated_image)\n",
        "        img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        face = align_and_crop_face_fm(img, face_landmarks, expand=1.3)\n",
        "        paintedFace = face2paint(img,512,False)\n",
        "        display(paintedFace)\n",
        "        save_path = image_name.rsplit('/',1)[0] + \"/Art/\" + image_name.rsplit('/',1)[1].partition('.')[0] + \"_out.jpg\"\n",
        "        paintedFace.save(save_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}